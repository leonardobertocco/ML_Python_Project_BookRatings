import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("books.csv",sep=",", on_bad_lines='skip')

df.head(10)

df.tail(10)

# Data Cleaning and Exploration

df.describe()

df.shape

df.dropna(axis = 0, how='any', inplace = True)
df.shape

from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype

is_string_dtype(df['publication_date'])

df['publication_year'] = df['publication_date'].str[-4:]

df.head(10)

df['publication_year'] = df['publication_year'].apply(int)
df.dtypes

#empty values check
plt.figure(figsize=(14,7))
sns.heatmap(df.isnull(), cbar=False)
sns.set_context('paper')

# Import warnings
warnings.simplefilter("ignore")

# Figure 1


sns.distplot(df['publication_year'])

df = df[(df['publication_year'] >= 1950) & (df['publication_year'] <= 2016)]
sns.distplot(df['publication_year'])

# Figure 2


sns.set_context('poster')
plt.figure(figsize=(20,15))
books = df['title'].value_counts()[:30]
rating = df.average_rating[:20]
sns.barplot(x = books, y = books.index, palette='deep')
plt.title("Most Occurring Books")
plt.xlabel("Number of occurances")
plt.ylabel("Books")
plt.show()

# Figure 3


sns.displot(df, x="average_rating", kde=True)

# Figure 4

counts = df['language_code'].value_counts().rename_axis('language_code').reset_index(name='count')

ax = sns.barplot(x='language_code', y='count', data=counts)
ax.figure.set_size_inches(40,20)
ax.bar_label(ax.containers[0])

# Figure 5


most_rated = df.sort_values('ratings_count', ascending = False).head(10).set_index('title')
plt.figure(figsize=(15,10))
sns.barplot(most_rated['ratings_count'], most_rated.index, palette='rocket')

# Figure 6


sns.set_context('talk')
most_books = df.groupby('authors')['title'].count().reset_index().sort_values('title', ascending=False).head(10).set_index('authors')
plt.figure(figsize=(15,10))
ax = sns.barplot(most_books['title'], most_books.index, palette='icefire_r')
ax.set_title("Top 10 authors with most books")
ax.set_xlabel("Total number of books")
for i in ax.patches:
    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')

duplicate = df.groupby('authors').count().sort_values('title', ascending=False).head(10)
 
print("Top 10 Duplicate Rows based on authors :")
 
# Print the resultant Dataframe
duplicate

# Figure 7


sns.set_context('talk')
most_books = df.groupby('publisher')['title'].count().reset_index().sort_values('title', ascending=False).head(10).set_index('publisher')
plt.figure(figsize=(15,10))
ax = sns.barplot(most_books['title'], most_books.index, palette='icefire_r')
ax.set_title("Top 10 Publisher with most books")
ax.set_xlabel("Total number of books")
for i in ax.patches:
    ax.text(i.get_width()+.3, i.get_y()+0.5, str(round(i.get_width())), fontsize = 10, color = 'k')

# Figure 8


df_v=df[['publication_year']].copy()
df_v['publication_year'] = df_v['publication_year'].astype(int).astype(str)
df_v=df_v['publication_year'].value_counts().head(25).reset_index()
df_v.columns=['year','count']
df_v['year']='Year '+df_v['year']

plt.figure(figsize=(10,8))
sns.barplot(x='count',y='year',data=df_v)
plt.ylabel('Year Of Publication')
plt.yticks(size=12)
plt.title('Top 25 Years of Publication :',size=20)
plt.show()

# Figure 9


import plotly.graph_objects as go
df_book_name=df.title.value_counts()[0:10].reset_index().rename(columns={'index':'title','title':'count'})

colors=['cyan','royalblue','blue','darkblue',"darkcyan",'Brown','Coral','OrangeRed','SaddleBrown','Tomato']
fig = go.Figure([go.Pie(labels=df_book_name['title'], values=df_book_name['count'])])
fig.update_traces(hoverinfo='label+percent', textinfo='percent+value', textfont_size=15,
                 marker=dict(colors=colors, line=dict(color='#000000', width=2)))
fig.update_layout(title="Most Reviewed Books ",title_x=0.3)
fig.show()



s = df.pop('average_rating')
df = pd.concat([df, s], 1)
df.head(100)

# Figure 10


plt.subplots(figsize=(15,10))
heat = sns.heatmap(df.corr(),annot=True) 

# Figure 11


fig = plt.gcf()
fig.set_size_inches(26, 10)
sns.lineplot(x="publication_year", y="average_rating", data=df)

fig = plt.gcf()
fig.set_size_inches(26, 10)
sns.lineplot(x="publication_year", y="text_reviews_count", data=df)

df['publication_date'] = pd.to_datetime(df['publication_date'], format='%m/%d/%Y', errors='coerce') # Convert data type of publication_date from object into date type
df[df['publication_date'].isnull()]

df.head(10)

# Figure 12

# Outliers removing method : 

import seaborn as sns
sns.boxplot(df['text_reviews_count'])

import seaborn as sns
sns.boxplot(df['ratings_count'])

import seaborn as sns
sns.boxplot(df['  num_pages'])

import warnings
warnings.filterwarnings('ignore')
plt.figure(figsize=(16,5))
plt.subplot(1,2,2)
sns.distplot(df['ratings_count'])
plt.show()   

df.dtypes

Removing outliers for ratings count

print(df["ratings_count"])

print("Highest allowed",df['ratings_count'].mean() + 3*df['ratings_count'].std())
print("Lowest allowed",df['ratings_count'].mean() - 3*df['ratings_count'].std())

df[(df['ratings_count'] < 0)]

df[(df['ratings_count'] > 356083.91)]

# Figure 13

Other outliers removing method :

upper_limit = df['ratings_count'].quantile(0.99)
lower_limit = df['ratings_count'].quantile(0.01)

print(upper_limit)
print(lower_limit)

df[(df['ratings_count'] > 298298.18)]

df.drop(df[df['ratings_count'] >= 298298.18].index, inplace = True)

df[(df['ratings_count'] > 298298.18)]

checking outliers for text_reviews_count : we don't need to remove car the most count of text reviews is 18865 which is logic

upper_limit = df['text_reviews_count'].quantile(0.99999999)
lower_limit = df['text_reviews_count'].quantile(0.01)

print(upper_limit)
print(lower_limit)

df[(df['text_reviews_count'] > 18864.66)]

Cheking outliers for pages number column

df[(df['  num_pages'] > 4000)]

df.drop(df[df['  num_pages'] >= 4000].index, inplace = True)

#df.drop('language_code', inplace=True, axis=1)

df.dtypes

df.head(10)

print(df['language_code'].unique())

index_names = df[ df['language_code'] == "ita" ].index
index_names = df[ df['language_code'] == "enm" ].index
index_names = df[ df['language_code'] == "lat" ].index
index_names = df[ df['language_code'] == "swe" ].index
index_names = df[ df['language_code'] == "rus" ].index
index_names = df[ df['language_code'] == "srp" ].index
index_names = df[ df['language_code'] == "nl" ].index
index_names = df[ df['language_code'] == "msa" ].index
index_names = df[ df['language_code'] == "glg" ].index
index_names = df[ df['language_code'] == "wel" ].index
index_names = df[ df['language_code'] == "ara" ].index
index_names = df[ df['language_code'] == "nor" ].index
index_names = df[ df['language_code'] == "tur" ].index
index_names = df[ df['language_code'] == "gla" ].index
index_names = df[ df['language_code'] == "ale" ].index



df.drop(index_names, inplace = True)

# Figure 14

print(df['language_code'].unique())

from sklearn.preprocessing import OrdinalEncoder

encoding = {'language_code':{'en-US': 'eng', 'en-GB': 'eng', 'en-CA': 'eng'}} # Unify the langauge codes
df.replace(encoding, inplace=True)

enc = OrdinalEncoder()
enc.fit(df[['language_code']])
df[['language_code']] = enc.fit_transform(df[['language_code']]) # Apply ordinal encoding on language_code to convert it into numerical column

print(df['language_code'].unique())

df.head(10)

enc = OrdinalEncoder()
enc.fit(df[['title']])
df[['title']] = enc.fit_transform(df[['title']]) # Apply ordinal encoding on language_code to convert it into numerical column

print(df['title'].unique())

enc = OrdinalEncoder()
enc.fit(df[['authors']])
df[['authors']] = enc.fit_transform(df[['authors']]) # Apply ordinal encoding on language_code to convert it into numerical column

enc = OrdinalEncoder()
enc.fit(df[['publisher']])
df[['publisher']] = enc.fit_transform(df[['publisher']]) # Apply ordinal encoding on language_code to convert it into numerical column

print(df['authors'].unique())

print(df['publisher'].unique())

df.head(10)

# Modeling

#Fonction de Normalisation
col_to_norm = ['  num_pages', 'ratings_count', 'text_reviews_count','publication_year','language_code','authors','title','publisher']
df[col_to_norm] = df[col_to_norm].apply(lambda x : (x-np.mean(x))/np.std(x))
df

label = df['average_rating'].values
df.drop(['average_rating'], axis=1, inplace=True)

#Split the Data into 70% - 30%
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

label = df['average_rating'].values
df.drop(['publication_date','isbn','isbn13','bookID','average_rating','publication_year'], axis=1, inplace=True)
df.round(2)

# Split the Data into 70% - 30%
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df, label, test_size=0.3)

# AdaBoostRegressor Using DecisionTreeRegressor

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV


model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4))

parameters = {
    'learning_rate': [0.001, 0.01, 0.02, 0.1, 0.2, 1.0],
    'n_estimators': [10, 50, 100, 150, 200, 250]
}

grad_Ada = RandomizedSearchCV(estimator = model, param_distributions = parameters, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs = -1)
grad_Ada.fit(X_train, y_train)
grad_Ada.best_params_

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

model = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4))

parameters = {
    'learning_rate': [0.005, 0.01, 0.015],
    'n_estimators': [140, 150, 160]
}

grad_Ada = GridSearchCV(model, parameters, refit=True, cv = 10, verbose=2, n_jobs = -1)
grad_Ada.fit(X_train, y_train)

print('Best Score: ', grad_Ada.best_score_, '\nBest Parameters: ', grad_Ada.best_params_)

# Resultat de  prédiction
y_pred = grad_Ada.predict(X_test)
y_pred

#on souhaite comparer les valeurs réels avec les valeurs 
y_pred = grad_Ada.predict(X_test)
y_pred
defaulter_decision_test  = y_pred 
df_pred = pd.DataFrame()

df_pred['prediction'] = defaulter_decision_test.astype(float)
df_pred['actual'] = y_test
df_pred['diff'] = df_pred['actual'] - df_pred['prediction']


display(df_pred.round(2))


# LinearRegression

pip show scikit-learn

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV

model  = LinearRegression()

parameters = {
    'fit_intercept': [True, False],
    'normalize': [True, False],
    
}

grad_Linear = GridSearchCV(model, parameters, refit=True, cv = 10)
grad_Linear.fit(X_train, y_train)

print('Best Score: ', grad_Linear.best_score_, '\nBest Parameters: ', grad_Linear.best_params_)

# Ridge

from sklearn.linear_model import Ridge

model = Ridge()

parameters = {
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'max_iter': [1000, 100, 10000],
    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
}

grad_ridge = GridSearchCV(model, parameters, refit=True)
grad_ridge.fit(X_train, y_train)

print('Best Score: ', grad_ridge.best_score_, '\nBest Parameters: ', grad_ridge.best_params_)

# ExtraTreesRegressor

from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor()

#n_estimators=23,criterion="gini",max_features="auto",random_state=131
parameters = {
    'n_estimators': [100, 200, 300, 400],
    'random_state': [10, 50, 100, 150, 200],
    'max_features':['auto'],
}

grad_ETR = RandomizedSearchCV(estimator = model, param_distributions = parameters, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs = -1)
grad_ETR.fit(X_train, y_train)
grad_ETR.best_params_

#results:
#1) Fitting 10 folds for each of 35 candidates, totalling 350 fits
#{'random_state': 200, 'n_estimators': 400, 'max_features': 'auto'}
#2) 
#

from sklearn.ensemble import ExtraTreesRegressor
    
model = ExtraTreesRegressor()
parameters = {
    'random_state': [190, 200, 210],
    'n_estimators': [390, 400, 410],
    'criterion':['mse']
    
}
grad_ETR = GridSearchCV(model, parameters,cv = 10, verbose=2, n_jobs = -1)
grad_ETR.fit(X_train, y_train)

print('Best Score: ', grad_ETR.best_score_, '\nBest Parameters: ', grad_ETR.best_params_)

#on souhaite comparer les valeurs réels avec les valeurs 
y_pred = grad_ETR.predict(X_test)
y_pred
defaulter_decision_test  = y_pred 
df_pred = pd.DataFrame()

df_pred['prediction'] = defaulter_decision_test.astype(float)
df_pred['actual'] = y_test
df_pred['diff'] = df_pred['actual'] - df_pred['prediction']


display(df_pred.round(2))


# RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()

parameters = {
    'n_estimators': [10, 50, 100, 150],
    'max_depth': [10, 15],
    'min_samples_split': [1, 5, 10, 15],
    'min_samples_leaf': [1, 5, 10, 15]
}

grad_rf = RandomizedSearchCV(estimator = model, param_distributions = parameters, n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)
grad_rf.fit(X_train, y_train)
grad_rf.best_params_

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()

parameters = {
    'n_estimators': [1000],
    'max_depth': [15],
    'min_samples_split': [15],
    'min_samples_leaf': [15],
    'criterion': ['mse'],
    'random_state': [0],
    'max_features': ['log2'],
    'bootstrap': ['False']
}

grad_rf = GridSearchCV(model, parameters, refit=True, cv=10)
grad_rf.fit(X_train, y_train)   

print('Best Score: ', grad_rf.best_score_, '\nBest Parameters: ', grad_rf.best_params_)

#Best Score:  0.21197633483447195 
#Best Parameters:  {'max_depth': 15, 'min_samples_leaf': 15, 'min_samples_split': 15, 'n_estimators': 100}

#on souhaite comparer les valeurs réels avec les valeurs 
y_pred = grad_rf.predict(X_test)
y_pred
defaulter_decision_test  = y_pred 
df_pred = pd.DataFrame()

df_pred['prediction'] = defaulter_decision_test.astype(float)
df_pred['actual'] = y_test
df_pred['diff'] = df_pred['actual'] - df_pred['prediction']


display(df_pred.round(2))


!pip install xgboost

# XGBRegressor

from sklearn import preprocessing
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import GridSearchCV

model = XGBRegressor()

parameters = {
    'nthread':[4], #when use hyperthread, xgboost may become slower
    'objective':['reg:linear'],
    'learning_rate': [0.001, 0.01, 0.02, 0.04, 0.05, 0.1, 0.2, 0.5], #so called `eta` value
    'max_depth': [5, 6, 7, 10, 15, 20, 25, 30, 40],
    'min_child_weight': [2, 4, 8, 16, 32, 64],
    'silent': [1, 2, 4, 8, 16, 32, 64],
    'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
    'n_estimators': [400, 500, 1000]
}

grad_xgb = RandomizedSearchCV(estimator = model, param_distributions = parameters, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs = -1)
grad_xgb.fit(X_train, y_train)
grad_xgb.best_params_

from sklearn import preprocessing
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.model_selection import GridSearchCV

# Various hyper-parameters to tune
model = XGBRegressor()
parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [0.01], #so called `eta` value
              'max_depth': [25],
              'min_child_weight': [32],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [1000]}

grad_xgb = GridSearchCV(model, parameters, cv = 10, verbose=2, n_jobs = -1)

grad_xgb.fit(X_train, y_train)   

print('Best Score: ', grad_xgb.best_score_, '\nBest Parameters: ', grad_xgb.best_params_)

#Result :   
#Best Score:  0.17096989196645165 
#Best Parameters:  {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 25, 'min_child_weight': 32, 'n_estimators': 500, 'nthread': 4, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.7}
#change n_estimators to 1000 = Best Score:  0.1751432519757135 

#on souhaite comparer les valeurs réels avec les valeurs 
y_pred = grad_xgb.predict(X_test)
y_pred
defaulter_decision_test  = y_pred 
df_pred = pd.DataFrame()

df_pred['prediction'] = defaulter_decision_test.astype(float)
df_pred['actual'] = y_test
df_pred['diff'] = df_pred['actual'] - df_pred['prediction']



display(df_pred.round(2))

from sklearn.metrics import balanced_accuracy_score
grad_xgb.score(X_test, y_test)

# XGBRegressor Model R^2
from sklearn.metrics import r2_score
r_squared = grad_xgb.score(X_train, y_train)
#view R-squared value
print(r_squared)
#This means that 58.40% of the variation in the Ratings can be explained by the other variables.

# XGBRegressor Model MSE
from sklearn.metrics import mean_squared_error
y_test_pred = grad_xgb.predict(X_test)
mse = mean_squared_error(y_test_pred, y_test)
mse

# XGBRegressor Model MSE
pred_rf = grad_xgb.predict(X_test)

# Check Model Score
print("Residual sum of squares: ",  np.mean((pred_rf - y_test) ** 2))
print('RMSE: '+str(np.sqrt(mean_squared_error(y_test, pred_rf))))
print('Model Score on Test Data: ', grad_rf.score(X_test, y_test))

plt.figure(figsize=(19,10))
sns.regplot(pred_rf,y_test, marker="+", line_kws={'color':'darkred','alpha':1.0})

df = []
df.append(('AdaBoost Regression', grad_Ada.score(X_test, y_test)*100))
df.append(('Linear Regression', grad_Linear.score(X_test, y_test)*100))
df.append(('Ridge Regression', grad_ridge.score(X_test, y_test)*100))
df.append(('ExtraTrees Regression', grad_ETR.score(X_test, y_test)*100))
df.append(('Random Forest Regression', grad_rf.score(X_test, y_test)*100))
df.append(('XGB Regression', grad_xgb.score(X_test, y_test)*100))
df

num = np.array(df)
reshaped = num.reshape(6,2)
pd.DataFrame(reshaped, columns=['Model','Test Score'])